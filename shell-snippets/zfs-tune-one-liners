## Adjust DNLC size

echo dnlc_max_nentries/W0t10485760 | mdb -kw
echo ncsize/W0t5240000 | mdb -kw

*****
* /etc/system changes applied by root using the ./NexTune.sh script
* on Tue May 24 02:06:50 CEST 2011
***
* set ZFS txg commit interval to 5 seconds from 30 seconds (default)
set zfs:zfs_txg_timeout=5

* set ZFS txg synctime target
set zfs:zfs_txg_synctime_ms=800

* set default SCSI command timeout to 20 seconds from 60 seconds (default)
set sd:sd_io_time=20

* * set default SCSI command retry count to 3 from 3 or 5 (default)
* set sd:sd_retry_count=3
* set TCP squeue policy to NODRAIN, helping performance for systems with 
* relatively few connections
set ip:tcp_squeue_wput=1
*****

## Adjusting ZFS write limit, min to 64G and max to 72GB
echo zfs_write_limit_min/Z0x1000000000|mdb -kw
echo zfs_write_limit_max/Z0x1200000000|mdb -kw


echo zfs_nocacheflush/W0t1 | mdb -kw
Revert to default:
echo zfs_nocacheflush/W0t0 | mdb -kw
Set the following parameter in the /etc/system file
set zfs:zfs_nocacheflush = 1


## ARC meta limit tuning in /etc/system file
set zfs:zfs_arc_meta_limit = <value in bytes or hex>

## Disable compression of metadata on disk
set zfs:zfs_mdcomp_disable = 1


## Real-time set txg_timeout to 5 seconds
# echo zfs_txg_timeout/W0t5 | mdb -kw

# echo zfs_txg_synctime_ms/W0t800 | mdb -kw

## Get all tunables with mdb 
echo "::zfs_params" | mdb -k

## Disable ZFS prefetch in ARC and l2arc
echo zfs_prefetch_disable/W0t1 | mdb -kw
echo l2arc_noprefetch/W0t1 | mdb -kw


############ L2ARC Stuff Here 
###############################
## Increase L2ARC write b/w set to 100MB/s
set zfs:l2arc_write_max = 104857600
set zfs:l2arc_write_max = 0x6400000

## Warm-up L2ARC devices more rapidly than usual, b/w is set to 100MB/s
echo "l2arc_write_boost/Z 0x6400000"|mdb -kw

## Set number of writers to L2ARC
echo "l2arc_headroom/W4"|mdb -kw

## Allow for reads to happen from L2ARC while writes are happening
echo "l2arc_norw/W0t0"|mdb -kw


## Quick check of arc_c_max and arc_meta_limit
# kstat zfs::arcstats:c_max && echo arc_meta_limit/E | mdb -k

# for a in zfs_txg_synctime_ms zfs_txg_timeout zfs_vdev_max_pending zfs_nocacheflush; do echo $a/D | mdb -k|grep [0-9] ; done

txg_synctime represents expected time to ï¬‚ush a TXG (5 sec)
txg_timeout represents max seconds worth of delta per txg

## Improve performance with pools at above 70% capacity. Deprecated in Illumos.
## Allows for for metaslabs with as little as 4K to be used.
echo "metaslab_min_alloc_size/Z 1000" | mdb -kw
set zfs:metaslab_min_alloc_size = 0x1000

## May improve performance with SSDs
set zfs:zfs_top_maxinflight = 64


## Memory 

## Each reference to a L2ARC entry requires an entry in ARC (ram)
# echo ::sizeof arc_buf_hdr_t | mdb -k

## Each entry in the DDT is a fixed size
# echo ::sizeof ddt_entry_t | mdb -k

## Get arc_meta_limit 
# echo "arc_meta_limit/G" | mdb -k

Set arc_meta_limit dynamically, this example sets it to 4GB
# echo "arc_meta_limit/Z 0x4000000" | mdb -kw
0x400000000
0x4000000

echo "arc_meta_limit/Z 0x400000000" | mdb -kw


Set to 48GB
# echo "arc_meta_limit/Z 0xc00000000" | mdb -kw

Set to 64GB
# echo "arc_meta_limit/Z 0x1000000000" | mdb -kw

Limit ARC max to 32GB of RAM
# echo "zfs_arc_max/Z 0x800000000" | mdb -kw 

## Enable no write_throttle
# echo zfs_no_write_throttle/W0t1|mdb -kw

## Kernel memory statistical breakdown
 echo ::kmastat | mdb -k

set zfs:arc_meta_limit = 0x1000000000

* Nexenta Support Recommended Parameters
* Disable ZFS Prefectch, based on poor hit rate against prefetched data
* 
set zfs:zfs_prefetch_disable = 0x1

* Nexenta Support Recommended Parameters
* Tuning arc_meta_limit, adjusting for large l2arc with headers 
* requiring additional space in ARC - setting limit to 24GB

set zfs:arc_meta_limit = 0x600000000
echo "arc_meta_limit/Z 0x600000000" | mdb -kw
* Set to 48GB
set zfs:arc_meta_limit = 0xC00000000
echo "arc_meta_limit/Z 0xC00000000" | mdb -kw

## Adjust ARC shrink shift
echo "arc_shrink_shift/W0t9"|mdb -kw

## Information about vdevs

## Output similar to zdb -l
echo "::spa -c" | mdb -k

## Check swap
"echo swapfs_minfree/D" | mdb -k

## Set dynamically minfree to 262144 pages page == 4096bytes

"echo swapfs_minfree/W0t262144" | mdb -kw

Centaur ZFS Tunables (/etc/system):
         set zfs:zfs_arc_shrink_shift = 11
         set zfs:zfs_arc_p_min_shift = 9
         set zfs:zfs_arc_meta_limit = 70000000000
         set zfs:zfs_txg_timeout = 1
         set zfs:zfs_txg_synctime_ms = 1000
         ** set zfs:zfs_no_write_throttle=1

"echo ::zfs_dbgmsg -v | mdb -k"

set zfs:zfs_prefetch_disable = 0x1
set zfs:zfs_arc_meta_limit = 8589934592


********************************************************************************
* Adjustments to scsi device driver defaults ***********************************
********************************************************************************
* Shorter timeout should allow us to fail in about 15 seconds, instead of
* the default 60s * 5 = 300s, which is far too long for any SAS/SATA disk
*
* set default SCSI command timeout to 5 seconds from 60 seconds (default)
* Setting applied on `date` per advise from Nexenta Support
set sd:sd_io_time = 5

* set default SCSI command retry count to 3 from 3 or 5 (default)
* Setting applied on `date` per advise from Nexenta Support
set sd:sd_retry_count = 3


# echo zfs_txg_synctime_ms/W0t2500 | mdb -kw; echo zfs_txg_timeout/W0t30 | mdb -kw

## Improving resilver performance
sys_zfs_resilver_delay  set to 1 from default of 2
sys_zfs_resilver_min_time : 2000 set from default of 3000
sys_zfs_vdev_max_pending=1 


## Default 3.0.5 standard
set zfs:zfs_txg_timeout = 30
set zfs:zfs_txg_synctime_ms = 5000
** set zfs:zfs_no_write_throttle=1

## Improve ZFS performance for pools above 80% full
echo "metaslab_df_free_pct/W 4" | mdb -kw
echo "set zfs:metaslab_df_free_pct=4" >> /etc/system

## Dupe queues for NFS tune to 8192
echo "cotsmaxdupreqs/W 0x2000" | mdb -kw
echo "maxdupreqs/W 0x2000" | mdb -kw

set rpcmod:maxdupreqs=8192 >> /etc/system
set rpcmod:cotsmaxdupreqs=8192 >> /etc/system

## Set write limit to 128MB, to reduce the size of possible flushes
set zfs:zfs_write_limit_override = 0x8000000 

Disable IDE module from loading with OS via /etc/system
exclude: pci-ide

## Enable cache on vdevs to improve metadata operations such as those from
## running commands like `zfs list -t snapshot`
echo zfs_vdev_cache_size/W0x1000000 | mdb -kw

## ZFS recovery flag, may prevent system from crashing

echo "aok/W 0x1" | mdb -kw
echo "zfs_recover/W 0x1" | mdb -kw

## With above options ZDB may fix some things
>> zdb -e -bcsvL <poolname>

## Maximize NFS throughput
set rpcmod:clnt_max_conns = 8

## Maximize throughput with few (<= # CPU / 2) TCP connections with heavy traffic:
set ip:tcp_squeue_wput=1

## Tune intel 10GbE Adapters, limiting interrupts, set in ixgbe.conf
intr_throttling=200;


## From Sun 7000 series tuning guide
* For NFS
set nfs:nfs3_max_threads=256 
set nfs:nfs4_max_threads=256 
set nfs:nfs3_nra=32 
set nfs:nfs4_nra=32 
set nfs:nfs3_bsize=1048576 
set nfs:nfs3_max_transfer_size=1048576 
*For NFS throughput 
set rpcmod:clnt_max_conns = 8 
set hires_tick=1  
*For Oracle Solaris ZFS 
set zfs:zfetch_max_streams=64 
set zfs:zfetch_block_cap=2048 
set zfs:zfs_txg_synctime=1 
set zfs:zfs_vdev_max_pending = 8  
*For IP Stack 
set ip:ip_squeue_fanout=1 
set ip:ip_squeue_bind=0 
set ip:ip_squeue_worker_wait=1

## Tune 10Gbit connections for NFS performance

ndd -set /dev/tcp tcp_recv_hiwat 400000                                
ndd -set /dev/tcp tcp_xmit_hiwat 400000
ndd -set /dev/tcp tcp_max_buf 2097152
ndd -set /dev/tcp tcp_cwnd_max 2097152